---
sidebar_position: 8
---

# Making pure Python dbt models

In this guide we will build a dbt model with a Python script using fal.

This guide is inspired by Andy Reagan's post where fal is mentioned, [Non-SQL in your DBT pipeline](https://andyreagan.medium.com/non-sql-in-your-dbt-pipeline-c7cef2091619). Which in turn is inspired by Claire Carroll's post on the dbt Discourse, [Representing non-SQL models in a dbt DAG](https://discourse.getdbt.com/t/representing-non-sql-models-in-a-dbt-dag/2083).

<!-- TODO: Why would we do this? -->
We love dbt, but it only runs SQL, some stuff can only (or much more easily) be achieved in other programming languages. In this case, we are going to use Python.

## Context

We are going to expand the jaffle shop classic example of dbt. We will add a life time value prediction to the shop costumers.

## Make dbt _know_ about the model

The first thing we want to do is to actually create a dbt model. 

```sql
{{ config(materialized='ephemeral') }}
/*
This table is generated by fal.

Please run this project with `fal flow` CLI tool instead of `dbt` to
get the correct results.

Specify script dependencies here for dbt and fal to know dependencies:

{{ ref('customers') }}
{{ ref('orders') }}
*/

-- Check Python file for real behaviour
SELECT * FROM {{ target.schema }}.{{ model.name }}
```
> File: `models/customers_predicted_ltvs.sql`

Things to note:

- Including the `ref`s in a comment means that the edges in your DAG will correctly drawn between other models and this non-SQL model
- Declaring this as an `ephemeral` model means dbt will not build this model
- Now this can be documented with `descriptions` in the `schema.yml` file

And let's add it to the schema.

```yaml
version: 2

models:
  - name: customers
    # ...
  - name: orders
    # ...
  - name: customers_predicted_ltvs
    description: Customers Lifetime value prediction
```
> File: `models/schema.yml`

## Make a fal script with the real model calculation

The script has to read the models it is interested in, calculate the new dataframe and write back to the data warehouse. 

```py
"""
Predict customer lifetime value
"""

from pandas import DataFrame
from utils import predict_ltv

# Make sure to include any `ref` here in the SQL file for correct DAG dependency
customers_df: DataFrame = ref("customers")
orders_df: DataFrame = ref("orders")

# For a defailed explanation on costumer lifetime value, 
# you can check out https://towardsdatascience.com/buy-til-you-die-predict-customer-lifetime-value-in-python-9701bfd4ddc0
customers_ltv_df: DataFrame = predict_ltv(customers_df, orders_df)

# It will use the script model as target table, no need to pass it
write_to_model(customers_ltv_df, mode="overwrite")
```
> File: `models/customers_predicted_ltvs.py`

And associate the script to the model in the schema.

```yaml
version: 2

models:
  - name: customers
    # ...
  - name: orders
    # ...
  - name: customers_predicted_ltvs
    description: Customers Lifetime value prediction
    meta:
      fal:
        scripts:
          after:
            - models/customers_predicted_ltvs.py
```
> File: `models/schema.yml`

## Reference the created model downstream

Now you can reference it from downstream models and it will know what to do.

```sql
with customers_predicted_ltv as (
  select * from {{ ref('customers_predicted_ltv') }}
),
...
```
> File: `models/fct_customers.sql`

<!-- TODO: add a picture of the docs with the node in there -->

## Run the models with fal flow

The last step is to test it. We are using the `--experimental-flow` flag to 
run fal after scripts right after a dbt node has finished to before continuing with downstream nodes.

```
$ fal flow run --experimental-flow
```
<!-- TODO: add a fal flow run out put here -->
